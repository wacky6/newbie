我的网络爬虫黑历史
===

[](~
    title:       我的网络爬虫黑历史
    ctime:       2016-08-03
    mtime:       2016-05-29
    tags:        技术
    keywords:    spider, crawl, web
    brief:       '1'
    description: '2'
~)

<style>
.hidden { visibility: hidden; }
</style>


## v0.1 最简单粗暴的方案
// original nju single file spider

## v0.7 并发控制
// revised nju spider

## v1.0 请求调度
// MDN Scrape

## v2.0 支持代理池 x 新浪新闻
代理池的需求起源于云计算期末大作业。需求是抓新浪新闻→分词统计→分析新闻热点。[真·从入门到被忽悠](< .hidden >)

考虑到需要获取的页面数量大，单IP获取会因为请求频率过高被封禁，需要把请求分散到多个IP上。再加上前段时间在某篇博客上看到了一篇代理池高并发爬虫的博客，就非常乐(zuo)意(si)的接下了数据收集的工作。

思路非常简单，先从某匿名代理列表上获取服务器列表，然后进行一次测试验证代理服务器可行。将这个列表提供给爬虫，用round-robin方式把请求分散到代理服务器上。

数据预处理方面，用Chrome的元素检查手动检查了新浪新闻的页面，发现新闻正文在`#artibody`元素下。于是用`html-to-text`提取并转换为txt存储。

#### 实测
从[西刺](http://www.xicidaili.com/)抓前5页列表。测试到新浪主页的联通情况，排除掉需要认证或响应过慢的代理。获得14个可接受的代理服务器。

并发请求数`16`，请求间隔`10ms`。峰值带宽可以跑满我的50Mbps小水管。

睡觉前让爬虫以深度4抓取，由于半开放的关系，记录已抓取的url消耗大量内存。睡醒后发现node进程已炸。获取的正文共540MB，远超预期的100MB。

因为新浪页面已经充分静态化，所以每个请求获取的实际有意义的数据并不多。另外，为了简(tou)化(lan)，抛弃了所有没有采用utf-8编码的页面。

[](< .hidden >) 该死的小组作业，绩点又被坑了 xD

## v3.0 分离式、分布式 x 围脖
// Sina weibo

至于如何发现、选择数据源，或许会开另一个坑。

[](< .hidden >)
小伙伴：我是围脖偷窥党。
我：爬完就存档，可爱的男孩纸的皂片一张都不能少。

